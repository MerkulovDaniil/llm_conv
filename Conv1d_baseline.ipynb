{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4t13JUZC-Ok",
        "outputId": "27f661b9-3529-4e08-fed1-5d84eb2aa139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdGM0iLC5NVl"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407,
          "referenced_widgets": [
            "0d1f15af26c24c129e9bfca45ebd895d",
            "01e8aa06b67d45c9aade04081cd0e93a",
            "9339f600ec874271b7a1f576b6811f5b",
            "e213d344a5444328b09689c6ef4c184a",
            "e86b7fc64d6242f388b04c3a08d923c6",
            "d8c80d4d443e42baa021097eb4a74ce5",
            "cf168ba5d0db40198cfc86b29055cf94",
            "de614dc601074120a73a918526468222",
            "17af7bffdd2744bcaeda5ffab10f5389",
            "756ded5836674aaeab30edc8ddf60209",
            "1c9f8f2b43b5487bb31989e028ad2af3",
            "53406853562a437388979641b1227325",
            "5e16672fe999484e91cf8cca67cd4edf",
            "0ab0809a2ca8409f934926acc39cda99",
            "fbcfa19965d144579d18cfd928e6be2d",
            "21aa591592654a87ad641ecdf959949a",
            "48b535e551a9421eb1bcc1bc4b88e693",
            "de5ce6fb47674f70b0c47b0d21a39721",
            "3846cdddfd614b359863981d5471fe26",
            "6fbb81fe8a3d48868ffa517d37e4a3c9",
            "a826caaf4e3547d8a7ebccabd24ab142",
            "5b73905e0e894a0e847a38754c319d4b"
          ]
        },
        "id": "g-UaswgYD6Ra",
        "outputId": "e0df5ecd-d813-4dfe-bf8e-6f6f8696811f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💎 Before training:\n",
            "GPU memory occupied: 5700 MB.\n",
            "💎 Dataset loaded\n",
            "GPU memory occupied: 5700 MB.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d1f15af26c24c129e9bfca45ebd895d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53406853562a437388979641b1227325",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💎 Model loaded\n",
            "GPU memory occupied: 5700 MB.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.3189377784729004, 'eval_runtime': 4.0651, 'eval_samples_per_second': 24.6, 'eval_steps_per_second': 3.198, 'epoch': 1.0}\n",
            "{'eval_loss': 2.273000717163086, 'eval_runtime': 4.1252, 'eval_samples_per_second': 24.241, 'eval_steps_per_second': 3.151, 'epoch': 2.0}\n",
            "{'eval_loss': 2.2519288063049316, 'eval_runtime': 4.1712, 'eval_samples_per_second': 23.974, 'eval_steps_per_second': 3.117, 'epoch': 3.0}\n",
            "{'eval_loss': 2.2411301136016846, 'eval_runtime': 4.203, 'eval_samples_per_second': 23.792, 'eval_steps_per_second': 3.093, 'epoch': 4.0}\n",
            "{'eval_loss': 2.2386133670806885, 'eval_runtime': 4.1096, 'eval_samples_per_second': 24.333, 'eval_steps_per_second': 3.163, 'epoch': 5.0}\n",
            "{'train_runtime': 105.6071, 'train_samples_per_second': 4.735, 'train_steps_per_second': 0.615, 'train_loss': 2.203471491887019, 'epoch': 5.0}\n",
            "💎 Training time: 105.61 seconds\n",
            "Samples/second: 4.74\n",
            "GPU memory occupied: 12080 MB.\n",
            "Total Trainable Parameters: 124439808\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "torch.cuda.empty_cache()  # Clears the cache\n",
        "torch.cuda.reset_peak_memory_stats()  #\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, \\\n",
        "    TrainingArguments, Trainer, logging, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from pynvml import *\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "print(\"💎 Before training:\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Suppress less critical logs\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Load the dataset with both training and evaluation splits\n",
        "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:100]\")\n",
        "eval_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[100:200]\")\n",
        "\n",
        "HF_cardname = \"openai-community/gpt2\"\n",
        "# HF_cardname = \"openai-community/gpt2-medium\"\n",
        "# HF_cardname = \"openai-community/gpt2-large\"\n",
        "# HF_cardname = \"openai-community/gpt2-XL\"\n",
        "# HF_cardname = \"facebook/opt-125m\"\n",
        "# HF_cardname = \"facebook/opt-350m\"\n",
        "# HF_cardname = \"facebook/opt-1.3b\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_cardname, use_fast=False)\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Ensure the tokenizer has a padding token, set EOS_TOKEN as padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = EOS_TOKEN\n",
        "\n",
        "# Function to process the dataset\n",
        "def formatting_func(examples):\n",
        "    inputs = [tokenizer(text + EOS_TOKEN, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\") for text in examples['text']]\n",
        "    return {'input_ids': [input['input_ids'].squeeze() for input in inputs], 'labels': [input['input_ids'].squeeze() for input in inputs]}\n",
        "\n",
        "# Process the datasets\n",
        "processed_train_dataset = train_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "processed_eval_dataset = eval_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Initialize Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "print(\"💎 Dataset loaded\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Define and load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(HF_cardname)\n",
        "\n",
        "print(\"💎 Model loaded\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Initialize the trainer with the data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_train_dataset,\n",
        "    eval_dataset=processed_eval_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "def print_summary(trainer):\n",
        "    # Access training result metrics directly from the trainer state\n",
        "    print(f\"💎 Training time: {trainer.state.log_history[-1]['train_runtime']:.2f} seconds\")\n",
        "    print(f\"Samples/second: {trainer.state.log_history[-1]['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "    num_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
        "    print(f\"Total Trainable Parameters: {num_params}\")\n",
        "\n",
        "print_summary(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfelptagsJpN",
        "outputId": "b950452c-2914-4b2a-9000-0f776ed05e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A long time ago in a galaxy far far away...\n",
            "\n",
            "A long time ago in a galaxy far away... A long time ago, a young girl named Lily was playing with her toy.\n",
            "\n",
            "Lily was so excited! She wanted to play with her toy! But she couldn't. She couldn't play with her toy. She couldn't play with her toy. She couldn't play with her toy. She couldn't play with her toy!\n",
            "\n",
            "Lily was so scared!\n"
          ]
        }
      ],
      "source": [
        "# Encode the prompt text\n",
        "input_ids = tokenizer.encode(\n",
        "    \"A long time ago in a galaxy far far away...\",\n",
        "    return_tensors=\"pt\").cuda()\n",
        "\n",
        "# Generate text using the model\n",
        "output_ids = model.generate(input_ids, max_length=100)\n",
        "\n",
        "# Decode the generated ids to text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeBHUnNJNvNL"
      },
      "source": [
        "# 🤖 EXPERIMENT: training gpt2-like model with conv1d filters from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "EWtz9nl6SpBU",
        "outputId": "11f228ac-884a-4533-cb5b-60a598d7253f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'input_ids' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-859b469dffb1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshort_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshort_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
          ]
        }
      ],
      "source": [
        "print(input_ids)\n",
        "short_ids = input_ids[:, :5]\n",
        "print(short_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m-seCeAO8-q",
        "outputId": "6cebace1-c528-40f9-975d-22828cd8377a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-32.2106, -31.9089, -34.7183, -34.6814, -33.1329, -33.6744, -32.2762,\n",
            "         -32.9669, -30.9345, -34.0401, -33.3299, -28.8423, -29.7336, -28.3824,\n",
            "         -31.4148, -33.5976, -32.4900, -32.8397, -33.2518, -33.3086, -33.6257,\n",
            "         -33.9383, -33.6765, -33.9673, -34.0025, -30.5542, -31.7219, -34.7232,\n",
            "         -33.8429, -34.1196, -32.4132, -34.9633, -32.8016, -33.3249, -33.7593,\n",
            "         -33.5905, -34.0067, -34.0373, -33.6612, -33.8630]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "tensor([[-32.2107, -31.9090, -34.7184, -34.6815, -33.1330, -33.6745, -32.2762,\n",
            "         -32.9670, -30.9346, -34.0401, -33.3300, -28.8423, -29.7336, -28.3825,\n",
            "         -31.4148, -33.5976, -32.4901, -32.8397, -33.2519, -33.3086, -33.6258,\n",
            "         -33.9384, -33.6766, -33.9674, -34.0026, -30.5543, -31.7220, -34.7233,\n",
            "         -33.8430, -34.1196, -32.4133, -34.9633, -32.8016, -33.3250, -33.7594,\n",
            "         -33.5906, -34.0068, -34.0374, -33.6613, -33.8631]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "tensor(0.0004, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "pred_vec1 = model(input_ids)[0][:, 0, :]\n",
        "pred_vec2 = model(short_ids)[0][:, 0, :]\n",
        "print(pred_vec1)\n",
        "print(pred_vec2)\n",
        "print(torch.linalg.norm(pred_vec1 - pred_vec2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0S6lihrNxs1",
        "outputId": "65c5a833-4ba0-4f3e-f7a9-53e2c3663ccb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(model(input_ids)[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwtP3akaVOXr",
        "outputId": "af92f981-cffe-483b-ab45-ebb435f192d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "2j3QcXokV0m0",
        "outputId": "00a7527f-bdc0-4582-b59a-b21227290128"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ec9361c4c7f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mlong\u001b[0m \u001b[0mtime\u001b[0m \u001b[0mago\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgalaxy\u001b[0m \u001b[0mfar\u001b[0m \u001b[0mfar\u001b[0m \u001b[0maway\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mlong\u001b[0m \u001b[0mtime\u001b[0m \u001b[0mago\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgalaxy\u001b[0m \u001b[0mfar\u001b[0m \u001b[0mfar\u001b[0m \u001b[0maway\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     return_tensors=\"pt\").cuda()\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "long_input = tokenizer.encode(\n",
        "    \"A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\\n",
        "    A long time ago in a galaxy far far away...\\.\",\n",
        "    return_tensors=\"pt\").cuda()\n",
        "\n",
        "print(long_input.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWkgERb1WNSd",
        "outputId": "c9398823-1bf4-4d98-862c-be08aea3ebfe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1202, 50257])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(long_input)[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG4kutMmWmBN",
        "outputId": "bb49871a-9378-40db-b314-4615bd98a3da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 1024\n"
          ]
        }
      ],
      "source": [
        "config = model.config\n",
        "\n",
        "# Maximum sequence length\n",
        "max_seq_len = config.n_positions\n",
        "print(f\"Maximum sequence length: {max_seq_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ0XgnRQgshY",
        "outputId": "2713af24-b2b4-438e-f36a-649673d40b19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💎 Before training:\n",
            "GPU memory occupied: 11808 MB.\n",
            "💎 Dataset loaded\n",
            "GPU memory occupied: 11808 MB.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💎 Evaluating model before training:\n",
            "{'eval_loss': 10.8544340133667, 'eval_runtime': 2.9063, 'eval_samples_per_second': 34.407, 'eval_steps_per_second': 4.473}\n",
            "Pre-training evaluation loss: 10.8544\n",
            "{'eval_loss': 10.120983123779297, 'eval_runtime': 3.0421, 'eval_samples_per_second': 32.872, 'eval_steps_per_second': 4.273, 'epoch': 1.0}\n",
            "{'eval_loss': 9.829243659973145, 'eval_runtime': 3.0655, 'eval_samples_per_second': 32.621, 'eval_steps_per_second': 4.241, 'epoch': 2.0}\n",
            "{'eval_loss': 9.551410675048828, 'eval_runtime': 2.9699, 'eval_samples_per_second': 33.671, 'eval_steps_per_second': 4.377, 'epoch': 3.0}\n",
            "{'eval_loss': 9.31762981414795, 'eval_runtime': 2.9191, 'eval_samples_per_second': 34.257, 'eval_steps_per_second': 4.453, 'epoch': 4.0}\n",
            "{'eval_loss': 9.231056213378906, 'eval_runtime': 2.867, 'eval_samples_per_second': 34.879, 'eval_steps_per_second': 4.534, 'epoch': 5.0}\n",
            "{'train_runtime': 81.6397, 'train_samples_per_second': 6.124, 'train_steps_per_second': 0.796, 'train_loss': 9.707406850961538, 'epoch': 5.0}\n",
            "💎 Training time: 81.64 seconds\n",
            "Samples/second: 6.12\n",
            "GPU memory occupied: 9924 MB.\n",
            "Total Trainable Parameters: 165398016\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Model, GPT2PreTrainedModel, GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, \\\n",
        "    TrainingArguments, Trainer, logging, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from pynvml import *\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def fix_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "fix_seed()  # Apply the fixed seed\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "print(\"💎 Before training:\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Suppress less critical logs\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Load the dataset with both training and evaluation splits\n",
        "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:100]\")\n",
        "eval_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[100:200]\")\n",
        "\n",
        "HF_cardname = \"openai-community/gpt2\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_cardname, use_fast=False)\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Ensure the tokenizer has a padding token, set EOS_TOKEN as padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = EOS_TOKEN\n",
        "\n",
        "# Function to process the dataset\n",
        "def formatting_func(examples):\n",
        "    inputs = [tokenizer(text + EOS_TOKEN, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\") for text in examples['text']]\n",
        "    return {'input_ids': [input['input_ids'].squeeze() for input in inputs], 'labels': [input['input_ids'].squeeze() for input in inputs]}\n",
        "\n",
        "# Process the datasets\n",
        "fix_seed()\n",
        "processed_train_dataset = train_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "processed_eval_dataset = eval_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Initialize Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "print(\"💎 Dataset loaded\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "class CustomGPT2Model(GPT2LMHeadModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # Define the custom layers\n",
        "        self.conv1d = nn.Conv1d(in_channels=config.hidden_size, out_channels=config.hidden_size, kernel_size=2, stride=2)\n",
        "        # self.conv1d = nn.Identity(27)\n",
        "        self.conv_transpose1d = nn.ConvTranspose1d(in_channels=config.hidden_size, out_channels=config.hidden_size, kernel_size=2, stride=2)\n",
        "        # self.conv_transpose1d = nn.Identity(27)\n",
        "\n",
        "        # GPT2Model is the transformer part of the GPT2 model\n",
        "        self.transformer = GPT2Model(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "        # Embedding layer\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(input_ids.size(-1), dtype=torch.long, device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        inputs_embeds = self.transformer.wte(input_ids)\n",
        "        position_embeds = self.transformer.wpe(position_ids)\n",
        "        hidden_states = inputs_embeds + position_embeds\n",
        "\n",
        "        # Custom conv1d layer: Reduce sequence length by half\n",
        "        hidden_states = hidden_states.permute(0, 2, 1)  # Change to shape (batch_size, emb_dim, seq_len)\n",
        "        hidden_states = self.conv1d(hidden_states)\n",
        "        hidden_states = hidden_states.permute(0, 2, 1)  # Change back to shape (batch_size, seq_len/2, emb_dim)\n",
        "\n",
        "        # Usual transformer forward pass on reduced sequence length\n",
        "        transformer_outputs = self.transformer(\n",
        "            inputs_embeds=hidden_states,  # Use the processed hidden states here\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=None,\n",
        "            head_mask=head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict\n",
        "        )\n",
        "\n",
        "        hidden_states = transformer_outputs[0]  # Output of the transformer\n",
        "\n",
        "        # Custom conv_transpose1d layer: Revert the sequence length to the original size\n",
        "        hidden_states = hidden_states.permute(0, 2, 1)  # Change to shape (batch_size, emb_dim, seq_len/2)\n",
        "        hidden_states = self.conv_transpose1d(hidden_states)\n",
        "        hidden_states = hidden_states.permute(0, 2, 1)  # Change back to shape (batch_size, seq_len, emb_dim)\n",
        "\n",
        "        # The final output projection layer\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + transformer_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return {'loss': loss, 'logits': lm_logits, 'hidden_states': transformer_outputs.hidden_states, 'attentions': transformer_outputs.attentions}\n",
        "\n",
        "# Initialize the custom model\n",
        "config = GPT2Config.from_pretrained(HF_cardname)\n",
        "model = CustomGPT2Model(config)\n",
        "\n",
        "# Tokenizer remains the same\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_cardname, use_fast=False)\n",
        "fix_seed()\n",
        "\n",
        "# Continue with the training setup as in your original code\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "fix_seed()\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_train_dataset,\n",
        "    eval_dataset=processed_eval_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Evaluate the model before training\n",
        "print(\"💎 Evaluating model before training:\")\n",
        "fix_seed()\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Pre-training evaluation loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "def print_summary(trainer):\n",
        "    # Access training result metrics directly from the trainer state\n",
        "    print(f\"💎 Training time: {trainer.state.log_history[-1]['train_runtime']:.2f} seconds\")\n",
        "    print(f\"Samples/second: {trainer.state.log_history[-1]['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "    num_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
        "    print(f\"Total Trainable Parameters: {num_params}\")\n",
        "\n",
        "print_summary(trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgZQQXRw1KJr",
        "outputId": "9c486836-4768-45d9-b2de-adc22eb93a48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(1024, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (h): ModuleList(\n",
              "    (0-11): 12 x GPT2Block(\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): GPT2SdpaAttention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): GPT2MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (act): NewGELUActivation()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzLt8W58puig"
      },
      "source": [
        "# 🤖 Experiment: training gpt2 model without HF trainer with explicit PyTorch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "jK49efqKp1Yn",
        "outputId": "986a7b6c-690e-49e6-a6e7-43fc591b8d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💎 Before training:\n",
            "GPU memory occupied: 12080 MB.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-fcce0a34673a>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Define the model using the custom architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_gpt2_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHF_cardname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-fcce0a34673a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGPT2Block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-fcce0a34673a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGPT2Block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-fcce0a34673a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         self.mlp = nn.Sequential(\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgelu_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AdamW, AutoConfig, AutoModelForCausalLM\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from datasets import load_dataset\n",
        "from pynvml import *\n",
        "\n",
        "# Custom GELU activation (gelu_new)\n",
        "def gelu_new(x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh(torch.sqrt(2.0 / torch.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "# Custom Transformer block with gelu_new activation\n",
        "class GPT2Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2Block, self).__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.attn = nn.MultiheadAttention(config.n_embd, config.n_head, dropout=config.attn_pdrop)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_inner),\n",
        "            nn.ReLU() if config.activation_function == \"relu\" else gelu_new,\n",
        "            nn.Linear(config.n_inner, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False):\n",
        "        attn_output, attn_weights = self.attn(self.ln_1(x), self.ln_1(x), self.ln_1(x), need_weights=output_attentions, attn_mask=attention_mask)\n",
        "        x = x + attn_output\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_weights\n",
        "\n",
        "# Custom GPT-2 Model definition\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.embed_positions = nn.Embedding(config.max_position_embeddings, config.n_embd)\n",
        "        self.layers = nn.ModuleList([GPT2Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        input_shape = input_ids.size()\n",
        "        device = input_ids.device\n",
        "\n",
        "        inputs_embeds = self.embed_tokens(input_ids)\n",
        "        position_ids = torch.arange(0, input_shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        position_embeds = self.embed_positions(position_ids)\n",
        "\n",
        "        hidden_states = inputs_embeds + position_embeds\n",
        "\n",
        "        for layer in self.layers:\n",
        "            hidden_states, _ = layer(hidden_states)\n",
        "\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "        logits = self.head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits\n",
        "        )\n",
        "\n",
        "\n",
        "# Load pretrained weights from Hugging Face\n",
        "def load_pretrained_gpt2_weights(model, pretrained_model_name):\n",
        "    # Load the pretrained model from Hugging Face\n",
        "    pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
        "\n",
        "    # Transfer the weights\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = pretrained_model.state_dict()\n",
        "\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "print(\"💎 Before training:\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "# Load the dataset with both training and evaluation splits\n",
        "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:100]\")\n",
        "eval_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[100:200]\")\n",
        "\n",
        "HF_cardname = \"openai-community/gpt2\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_cardname, use_fast=False)\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Ensure the tokenizer has a padding token, set EOS_TOKEN as padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = EOS_TOKEN\n",
        "\n",
        "# Function to process the dataset\n",
        "def formatting_func(examples):\n",
        "    inputs = [tokenizer(text + EOS_TOKEN, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\") for text in examples['text']]\n",
        "    return {'input_ids': torch.stack([input['input_ids'].squeeze() for input in inputs]),\n",
        "            'labels': torch.stack([input['input_ids'].squeeze() for input in inputs])}\n",
        "\n",
        "# Process the datasets\n",
        "processed_train_dataset = train_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "processed_eval_dataset = eval_dataset.map(formatting_func, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(processed_train_dataset, batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(processed_eval_dataset, batch_size=8)\n",
        "\n",
        "# Load the configuration for GPT-2\n",
        "config = AutoConfig.from_pretrained(HF_cardname)\n",
        "\n",
        "# Define the model using the custom architecture\n",
        "model = GPT2Model(config)\n",
        "model = load_pretrained_gpt2_weights(model, HF_cardname)\n",
        "model = model.cuda()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "\n",
        "print(\"💎 Model and optimizer initialized\")\n",
        "print_gpu_utilization()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    running_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch['input_ids'].cuda()\n",
        "        labels = batch['labels'].cuda()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(input_ids=inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_dataloader)\n",
        "    print(f\"Training loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation step\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            inputs = batch['input_ids'].cuda()\n",
        "            labels = batch['labels'].cuda()\n",
        "\n",
        "            outputs = model(input_ids=inputs, labels=labels)\n",
        "            eval_loss += outputs.loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
        "    print(f\"Validation loss: {avg_eval_loss:.4f}\")\n",
        "    model.train()\n",
        "\n",
        "# Training summary\n",
        "def print_summary():\n",
        "    print(\"💎 Training completed\")\n",
        "    print_gpu_utilization()\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total Trainable Parameters: {num_params}\")\n",
        "\n",
        "print_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b69yqNOjEPk2"
      },
      "source": [
        "# NanoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vqM1aKNERGp",
        "outputId": "d31cc493-679a-4b38-88c9-354b8d248108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_2.weight torch.Size([768])\n",
            "transformer.h.1.ln_2.bias torch.Size([768])\n",
            "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_1.weight torch.Size([768])\n",
            "transformer.h.2.ln_1.bias torch.Size([768])\n",
            "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_2.weight torch.Size([768])\n",
            "transformer.h.2.ln_2.bias torch.Size([768])\n",
            "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_1.weight torch.Size([768])\n",
            "transformer.h.3.ln_1.bias torch.Size([768])\n",
            "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_2.weight torch.Size([768])\n",
            "transformer.h.3.ln_2.bias torch.Size([768])\n",
            "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_1.weight torch.Size([768])\n",
            "transformer.h.4.ln_1.bias torch.Size([768])\n",
            "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_2.weight torch.Size([768])\n",
            "transformer.h.4.ln_2.bias torch.Size([768])\n",
            "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_1.weight torch.Size([768])\n",
            "transformer.h.5.ln_1.bias torch.Size([768])\n",
            "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_2.weight torch.Size([768])\n",
            "transformer.h.5.ln_2.bias torch.Size([768])\n",
            "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_1.weight torch.Size([768])\n",
            "transformer.h.6.ln_1.bias torch.Size([768])\n",
            "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_2.weight torch.Size([768])\n",
            "transformer.h.6.ln_2.bias torch.Size([768])\n",
            "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_1.weight torch.Size([768])\n",
            "transformer.h.7.ln_1.bias torch.Size([768])\n",
            "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_2.weight torch.Size([768])\n",
            "transformer.h.7.ln_2.bias torch.Size([768])\n",
            "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_1.weight torch.Size([768])\n",
            "transformer.h.8.ln_1.bias torch.Size([768])\n",
            "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_2.weight torch.Size([768])\n",
            "transformer.h.8.ln_2.bias torch.Size([768])\n",
            "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_1.weight torch.Size([768])\n",
            "transformer.h.9.ln_1.bias torch.Size([768])\n",
            "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_2.weight torch.Size([768])\n",
            "transformer.h.9.ln_2.bias torch.Size([768])\n",
            "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_1.weight torch.Size([768])\n",
            "transformer.h.10.ln_1.bias torch.Size([768])\n",
            "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_2.weight torch.Size([768])\n",
            "transformer.h.10.ln_2.bias torch.Size([768])\n",
            "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_1.weight torch.Size([768])\n",
            "transformer.h.11.ln_1.bias torch.Size([768])\n",
            "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_2.weight torch.Size([768])\n",
            "transformer.h.11.ln_2.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, set_seed, pipeline\n",
        "\n",
        "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "sd_hf = model_hf.state_dict()\n",
        "\n",
        "for k, v in sd_hf.items():\n",
        "    print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6v8TkadFimp",
        "outputId": "bc53920b-9757-4e26-80df-b5bb4ffeb635"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'No, Luke, I am your ___________!\"\\n\\n\"Get up!\" I yelled, dragging my feet with me. I wasn\\'t the'},\n",
              " {'generated_text': 'No, Luke, I am your \\xa0friend.\" You had to come after him. What about this?!\\n\"There\\'s no turning back…'},\n",
              " {'generated_text': 'No, Luke, I am your ______________.\\n\\nYour father says, \"Luke! I need you to go to that place and'},\n",
              " {'generated_text': 'No, Luke, I am your ~~\"\\n\\n\"And you\\'re not my, aha!!\" Aang roared\\n\\nHis face turned'},\n",
              " {'generated_text': 'No, Luke, I am your _____, all of you!\"\\n\\n\"Oh, God! How dare you let even a tiny baby!\"'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"No, Luke, I am your \", max_length=30, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMpgtcwuGE2U"
      },
      "source": [
        "## gpt2 from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RtPNoFWnGHMq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "using device: mps\n",
            "> Без труда не вытащишь рыбку из ти велецино \n",
            "> Без труда не вытащишь рыбку из травно идена �\n",
            "> Без труда не вытащишь рыбку из что исчи зн\n",
            "> Без труда не вытащишь рыбку из сератика и гр\n",
            "> Без труда не вытащишь рыбку из струда сольно�\n",
            "> Без труда не вытащишь рыбку из собойскогахи\n",
            "> Без труда не вытащишь рыбку из седбломано п\n",
            "> Без труда не вытащишь рыбку из шка не венед\n",
            "> Без труда не вытащишь рыбку из этрын врезы\n",
            "> Без труда не вытащишь рыбку из сомаючие, д\n",
            "> Без труда не вытащишь рыбку из убрашимова.\n",
            "> Без труда не вытащишь рыбку из ули всото сво\n",
            "> Без труда не вытащишь рыбку из удиниительно �\n",
            "> Без труда не вытащишь рыбку из тутольком, от\n",
            "> Без труда не вытащишь рыбку из рейчных с �\n",
            "> Без труда не вытащишь рыбку из рекжиция ут\n",
            "> Без труда не вытащишь рыбку из чтоть мудово�\n",
            "> Без труда не вытащишь рыбку из фесноюнияе,\n",
            "> Без труда не вытащишь рыбку из треженикией\n",
            "> Без труда не вытащишь рыбку из рездичние м\n",
            "> Без труда не вытащишь рыбку из чаратела вас\n",
            "> Без труда не вытащишь рыбку из струда обель\n",
            "> Без труда не вытащишь рыбку из эленок зинко�\n",
            "> Без труда не вытащишь рыбку из редини и коч\n",
            "> Без труда не вытащишь рыбку из этябы оъли\n",
            "> Без труда не вытащишь рыбку из ропудо однико�\n",
            "> Без труда не вытащишь рыбку из с докуть нзд\n",
            "> Без труда не вытащишь рыбку из страйтичаст\n",
            "> Без труда не вытащишь рыбку из степобрияци\n",
            "> Без труда не вытащишь рыбку из ражит свяца\n",
            "> Без труда не вытащишь рыбку из рюдили одеди\n",
            "> Без труда не вытащишь рыбку из это ский вс\n",
            "> Без труда не вытащишь рыбку из справого. Се\n",
            "> Без труда не вытащишь рыбку из трустаєия х\n",
            "> Без труда не вытащишь рыбку из укиборащи в\n",
            "> Без труда не вытащишь рыбку из струда проша\n",
            "> Без труда не вытащишь рыбку из язвухер об\n",
            "> Без труда не вытащишь рыбку из тогигорудьны\n",
            "> Без труда не вытащишь рыбку из сельно сельна\n",
            "> Без труда не вытащишь рыбку из раголу чето�\n",
            "> Без труда не вытащишь рыбку из разокой обл\n",
            "> Без труда не вытащишь рыбку из учнога угл�\n",
            "> Без труда не вытащишь рыбку из укущиский �\n",
            "> Без труда не вытащишь рыбку из странорая не �\n",
            "> Без труда не вытащишь рыбку из роторых.\n",
            "\n",
            "Б\n",
            "> Без труда не вытащишь рыбку из эет ітел пр\n",
            "> Без труда не вытащишь рыбку из удет. Одение �\n",
            "> Без труда не вытащишь рыбку из ідонниство к\n",
            "> Без труда не вытащишь рыбку из что бетев в �\n",
            "> Без труда не вытащишь рыбку из свые поргал \n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import tiktoken\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # to split embeddings dimension across attention heads\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                             .view(1, 1, config.block_size, config.block_size))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be \n",
        "        # nh is \"number of heads\", hs is \"head size\" and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M) n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # attention (materializes the large (T, T) matrix for all the queries and keys)\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0/math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "        # y = att @ v\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h  = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "            ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is {self.config.block_size}\"\n",
        "        # forward the token and position embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position emeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token emeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        return logits\n",
        "    \n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "model = GPT.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "\n",
        "# attempt to autodetect device\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Без труда не вытащишь рыбку из \")\n",
        "batch_size = 50\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(batch_size, 1)\n",
        "x = tokens.to(device)\n",
        "\n",
        "max_length = 50\n",
        "torch.manual_seed(228)\n",
        "while x.size(1) < max_length:\n",
        "    logits = model(x)\n",
        "    # take the logits at the last position\n",
        "    logits = logits[:, -1, :]\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # do top-k sampling of 50 (default HF pipeline)\n",
        "    topk_probs, topk_indices = torch.topk(probs, 10, dim=-1)\n",
        "    # select a token from the top-k probabilities\n",
        "    ix = torch.multinomial(topk_probs, 1)\n",
        "    # gather the corresponding indices\n",
        "    xcol = torch.gather(topk_indices, -1, ix)\n",
        "    # append to the sequence\n",
        "    x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(batch_size):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sdGM0iLC5NVl"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01e8aa06b67d45c9aade04081cd0e93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c80d4d443e42baa021097eb4a74ce5",
            "placeholder": "​",
            "style": "IPY_MODEL_cf168ba5d0db40198cfc86b29055cf94",
            "value": "model.safetensors: 100%"
          }
        },
        "0ab0809a2ca8409f934926acc39cda99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3846cdddfd614b359863981d5471fe26",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fbb81fe8a3d48868ffa517d37e4a3c9",
            "value": 124
          }
        },
        "0d1f15af26c24c129e9bfca45ebd895d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01e8aa06b67d45c9aade04081cd0e93a",
              "IPY_MODEL_9339f600ec874271b7a1f576b6811f5b",
              "IPY_MODEL_e213d344a5444328b09689c6ef4c184a"
            ],
            "layout": "IPY_MODEL_e86b7fc64d6242f388b04c3a08d923c6"
          }
        },
        "17af7bffdd2744bcaeda5ffab10f5389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c9f8f2b43b5487bb31989e028ad2af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21aa591592654a87ad641ecdf959949a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3846cdddfd614b359863981d5471fe26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b535e551a9421eb1bcc1bc4b88e693": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53406853562a437388979641b1227325": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e16672fe999484e91cf8cca67cd4edf",
              "IPY_MODEL_0ab0809a2ca8409f934926acc39cda99",
              "IPY_MODEL_fbcfa19965d144579d18cfd928e6be2d"
            ],
            "layout": "IPY_MODEL_21aa591592654a87ad641ecdf959949a"
          }
        },
        "5b73905e0e894a0e847a38754c319d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e16672fe999484e91cf8cca67cd4edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48b535e551a9421eb1bcc1bc4b88e693",
            "placeholder": "​",
            "style": "IPY_MODEL_de5ce6fb47674f70b0c47b0d21a39721",
            "value": "generation_config.json: 100%"
          }
        },
        "6fbb81fe8a3d48868ffa517d37e4a3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "756ded5836674aaeab30edc8ddf60209": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9339f600ec874271b7a1f576b6811f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de614dc601074120a73a918526468222",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17af7bffdd2744bcaeda5ffab10f5389",
            "value": 548105171
          }
        },
        "a826caaf4e3547d8a7ebccabd24ab142": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf168ba5d0db40198cfc86b29055cf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8c80d4d443e42baa021097eb4a74ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5ce6fb47674f70b0c47b0d21a39721": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de614dc601074120a73a918526468222": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e213d344a5444328b09689c6ef4c184a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_756ded5836674aaeab30edc8ddf60209",
            "placeholder": "​",
            "style": "IPY_MODEL_1c9f8f2b43b5487bb31989e028ad2af3",
            "value": " 548M/548M [00:06&lt;00:00, 30.1MB/s]"
          }
        },
        "e86b7fc64d6242f388b04c3a08d923c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbcfa19965d144579d18cfd928e6be2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a826caaf4e3547d8a7ebccabd24ab142",
            "placeholder": "​",
            "style": "IPY_MODEL_5b73905e0e894a0e847a38754c319d4b",
            "value": " 124/124 [00:00&lt;00:00, 9.04kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
